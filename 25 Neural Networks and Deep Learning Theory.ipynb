{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 25 Neural Networks and Deep Learning Theory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "127. Perceptron Theory\n",
    "\n",
    "They are modeling real neurons as a way to introduce ANNs. \n",
    "Frank Rosenblatt and the idea of a Perceptron in 1958\n",
    "1969 Minsky and Papert publish the book Perceptron which highlights lack of computing power necessary for perceptrons. \n",
    "\n",
    "Preceptron theory:\n",
    "inputs: x1 and x2 go into a neuron which performs function f(x) and makes output y.\n",
    "Can modify the weights w1 and w2.\n",
    "You can add in a bias term b. \n",
    "y = (x1w1+b) + (x2w2+b)\n",
    "\n",
    "Tensor is an n dimensional matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "128. Neural Networks Theory\n",
    "\n",
    "Connect layers of perceptrons to build mutli-layer perceptron model.\n",
    "input layer receives the data. Output layer gives you an answer. Hidden layers are difficult to interpret due to high connectivity and distance from known input and output values. \n",
    "\n",
    "Deep neural nets must have 2 or more hidden layers. \n",
    "Neural networks can approximate any convex continuous function. \n",
    "We will want to set constraints for our output values especially for classification tasks. Such as outputs fall between 0 and 1 for a probability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "129. Activation Functions\n",
    "\n",
    "x*w+b\n",
    "b is a sort of offset value. If it is negative it can create a threshold which xw would have to over come to have an affect on the neuron. \n",
    "\n",
    "z = xw+b and pass z through some function to constrain its value. So the activation function would be f(z). If the value is capitalized that usually means you have a tensor input consisting of multiple falues.\n",
    "\n",
    "Activation Functions:\n",
    "\n",
    "Step Function - when z > 0 you output 1. Output 0 otherwise. Fails to reflect small changes. \n",
    "\n",
    "Sigmoid Function - bound by 0 and 1.  1/(1+e^(-z)). y=0.5 @ x=0\n",
    "\n",
    "Hyperbolic Tangent - sigmoid shape bound by -1 to 1. \n",
    "\n",
    "Rectified Linear Unit - ReLU - outputs 0 for all z values <0. For all z values >0 it ouputs the actual z value. Better at dealing with the \"vanishing gradient\" \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "130. Multi-clasas Classification Considerations\n",
    "\n",
    "Multi-class classification functions will have multiple neurons in the output layer. A single neuron in the output layer allows you to classify things as one of two classes (0 or 1) or as a continuous variable. \n",
    "\n",
    "Two types of multi-class situations:\n",
    "1) Non-exclusive classes - a data point can be assigned to multiple categories. Photo tags. \n",
    "2) Mutually exclusive classes - each data point has only a single class. Color or b/w photo. \n",
    "\n",
    "Organizing multipl classes - easiest way is to have one output node per class. \n",
    "\n",
    "How do we transform our data correctly to feed it into the neural network for multiclass coding? You cant just feed the neural network strings that say 'red' 'greeb' 'blue' etc..\n",
    "\n",
    "For mutually exclusive classes you can use:  One Hot Coding - binary classification for each classification.\n",
    "\n",
    "For non-exclusive classes - same as one hot coding except that each data point can have a value of 1 for multiple classes. \n",
    "\n",
    "\n",
    "Choose the correct classification activation function for the output layer:\n",
    "\n",
    "For non-exclussives you can just use a sigmoid function which indicates the probability that a data point has to belonging to each class. Usually set a probability threshold to determine which categories a data point qualifies for. \n",
    "\n",
    "For mutually exclusives you can use the Soft Max activation function. Calculates probability distribution over k events (k=# of classes). The sum of all probabilities is equal to 1. The category with the highest probability is then chosen. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "131. Cost Functions and Gradient Descent\n",
    "\n",
    "Cost Functions - allows you to measure how far off are you in the output of your neural network. Should be an average.\n",
    "Gradient Descent - helps minimize cost function (error)\n",
    "\n",
    "This process involves the estimating error between a (output) and y (label). Takes place during training/fitting. \n",
    "w*x+b=z and sigmoid(z)=a\n",
    "\n",
    "Quadratic Cost Function - similar to RMSE but adapted for multi-dim data. Simply estimate difference between real values, y(x), against the predicted values. a(x).\n",
    "C=cost\n",
    "C = (1/2n) Sum(y(x)-a(x))^2     squared errors punish large errors and accounts for +/- errors. a(x) encodes info about W and B. \n",
    "C(W,B,S,E)  W=nn wights, B=bias, S=output of single training sample, E=desired output of that training sample. \n",
    "\n",
    "In a large network C will be very complex with a large tensor for W and another one for B. So, how do you calculate this? \n",
    "\n",
    "Need to figure out which weights lead to the lowest cost? Taking the derivative of C(w) is one way but is far to computationally expensive.    Gradient Descent  - stochastic version to find minimum in C(w). Calculate slope at one point and move downward in that direction. Trade-off between larger and smaller step sizes. Step size = learning rate. Small rate can take too long. Large rate can fail to converge on a minimum. Adaptive Gradient Descent - adapt your step size based on the gradient you get back. One adaptive step size method is called \"Adam\"\n",
    "\n",
    "Calculating gradient descent in a hyper space of n dimensions. When dealing with n-dim data the notation changes from \"derivative\" to \"gradient\"\n",
    "\n",
    "Cross Entropy - loss function often used for classification problems. Assumes that your model predicts a probability distribution p(y=i) for each class i=1,2,...C.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "132. Backpropagation\n",
    "\n",
    "Consder a simple network with 4 layers and 1 neuron per layer. L-4, L-3, L-2, L-1, L. \n",
    "This means that Z(L) = W(L)*a(L-1)+b(L)  and   a(L) =sigmoid(Z(L))   and   C0(...)=(a(L)-y)^2\n",
    "\n",
    "How sensitive is the cost function to changes in w? Apply the chain rule from calculus to determine the partial derivative of the cost function with respect to the weights. Will also calculate the partial derivative of the cost function with respect to changes in the bias in L. \n",
    "\n",
    "From here on x will usually refer to the raw input. \n",
    "\n",
    "To forward propagate you are simply computing the z then the a for that layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
